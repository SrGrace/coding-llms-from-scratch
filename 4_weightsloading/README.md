#### Loading pretrained weights

Since pretraining is a long and expensive process, we will now load pretrained weights into our self-implemented architecture. 
Then, we will introduce the LitGPT open-source library, which provides more sophisticated (but still readable) code for training and finetuning LLMs. 
We will learn how to load weights of pretrained LLMs (Llama, Phi, Gemma, Mistral) in LitGPT.
